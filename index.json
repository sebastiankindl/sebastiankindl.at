[{"content":" The Challenge Predicting the physical properties of materials like Tungsten Diselenide (WSe2) usually requires Density Functional Theory (DFT). While highly accurate, DFT is computationally extremely expensive, limiting simulations to small systems and short timescales. Classical force fields, on the other hand, are fast but often fail to capture the complex quantum mechanical interactions of 2D materials.\nEstablishing the Reference SystemGround-truth phonon band structure and Density of States (DOS) for pristine WSe2. These vibrational properties serve as the physical reference used to validate the machine learning model\nThe Solution In my bachelor thesis, I implemented and trained a Machine Learning Force Field (MLFF) based on the MACE (Multi-Atomic Cluster Expansion) architecture.\nData Generation: Curated a high-quality dataset of atomic configurations using ab-initio methods. Model Training: Utilized equivariant neural networks to map atomic environments to potential energy surfaces. Validation: Benchmarked the model against ground-truth DFT data to ensure high fidelity in force and energy predictions. Key Results Efficiency: Achieved a speedup of several orders of magnitude compared to traditional DFT. Accuracy: Maintained \u0026ldquo;ab-initio\u0026rdquo; level precision (low MAE in energy/forces), enabling large-scale Molecular Dynamics (MD) simulations. Scalability: Demonstrated that the model can generalize to larger supercells that were previously unreachable. Scientific Insight: Successfully modeled defect-induced changes in the material\u0026rsquo;s vibrational spectrum, which is critical for semiconductor applications. Pristine vs DefectiveComparison of phonon band structures and PDOS, demonstrating the emergence of localized vibrational modes and confirming the model\u0026rsquo;s consistency with first-principles theory.\nRadial Phonon DOSRadial Comparison between pristine and defective system, revealing how vibrational properties evolve with distance from the defect. This demonstrates the model\u0026rsquo;s ability to capture localized physical interactions.\nTech Stack Languages: Python ML Frameworks: PyTorch, MACE Simulation Tools: ASE, Phonopy Science: DFT, Solid State Physics, Phonon Analysis View Code on GitHub ","permalink":"https://sebastiankindl.at/projects/mace-wse2/","summary":"Transforming computationally intensive problems into scalable solutions using AI","title":"AI-Based Framework for Scalable Defect Analysis in 2D Semiconductors"},{"content":" The Lead Audit DashboardA look at the final interface: The dashboard shows real-time processing, lead concentration metrics, and a detailed breakdown of strategic signals for each prospect.\nWhat problem does this solve? Most e-commerce teams are reactive - they analyze last month and blast campaigns broadly. This project turns customer analytics into a decision engine:\nPredict which customers will generate revenue in the next 90 days Explain the drivers globally and per-customer (SHAP) Optimize how to spend a fixed budget for maximum expected uplift (and quantify risk) What I built (end-to-end) Pipeline\nGenerates a realistic customer + transaction dataset (or uses provided CSVs) Builds RFM + behavioral features (recency, frequency, monetary, AOV, tenure, inter-purchase time) Trains an XGBoost regressor to predict 90-day revenue Produces evaluation artifacts (metrics + SHAP global importance) App\nExecutive dashboard (revenue estimate + model metrics) Drivers of customer value (global SHAP) Customer explorer (local explanation) ROI simulator + decision optimization Sensitivity analysis + Monte Carlo risk analysis Key results (what a recruiter should notice) Top-decile lift ~5x: the top 10% predicted customers capture ~50% of future revenue (typical CLV “power law” pattern) Strong explainability: business-readable drivers (monetary, AOV, frequency, recency, etc.) Prescriptive layer: converts predictions into budget decisions (ROI, efficient frontier, risk) Dashboard walkthrough 1) Drivers of Customer Value (Global SHAP) The Lead Audit DashboardA look at the final interface: The dashboard shows real-time processing, lead concentration metrics, and a detailed breakdown of strategic signals for each prospect.\n2) Customer Explorer (Local explanation) The Lead Audit DashboardA look at the final interface: The dashboard shows real-time processing, lead concentration metrics, and a detailed breakdown of strategic signals for each prospect.\n3) ROI Simulator \u0026amp; Optimization The Lead Audit DashboardA look at the final interface: The dashboard shows real-time processing, lead concentration metrics, and a detailed breakdown of strategic signals for each prospect.\nNotes \u0026amp; Limitations The model is predictive (correlation), not causal. ROI and uplift are assumption-driven simulations → should be validated with A/B tests / RCTs. The optimization is only as good as the uplift + cost assumptions. View Code on GitHub Live demo ","permalink":"https://sebastiankindl.at/projects/customer-analytics/","summary":"A production-style CLV system: prediction (XGBoost), explainability (SHAP), and prescriptive budget optimization with sensitivity and Monte Carlo risk analysis.","title":"CLV Engine for Predictive Customer Analytics"},{"content":" The Challenge Sales teams often face a high volume of inbound leads, but not all leads are created equal. Manually filtering through them is time-consuming and often leads to \u0026ldquo;sales fatigue\u0026rdquo; and missed opportunities. The goal was to build a system that automatically ranks leads based on their likelihood to convert, allowing the sales team to focus their energy where it matters most.\nThe Solution I developed an end-to-end Machine Learning pipeline to predict lead conversion (binary classification).\nData Engineering: Cleaned and preprocessed customer behavioral data, handling missing values and categorical encoding. Feature Engineering: Extracted insights from lead origins, website activity, and engagement metrics to create predictive features. Model Development: Evaluated several classification algorithms (Logistic Regression, Random Forest, and XGBoost) to find the best balance between precision and recall. Optimization: Focused on minimizing \u0026ldquo;False Positives\u0026rdquo; to ensure the sales team doesn\u0026rsquo;t waste time on cold leads. The Lead Audit DashboardA look at the final interface: The dashboard shows real-time processing, lead concentration metrics, and a detailed breakdown of strategic signals for each prospect.\nKey Results Prioritization: The model successfully identified the top 20% of leads that accounted for over 80% of potential conversions. Actionable Insights: Discovered that specific engagement triggers (e.g., time spent on the pricing page) were stronger predictors than traditional demographic data. Performance: Achieved a high AUC-ROC score, demonstrating robust discriminatory power between converting and non-converting leads. Tech Stack Languages: Python Libraries: Pandas, NumPy, Scikit-Learn, Matplotlib/Seaborn Algorithms: Random Forest, XGBoost, Logistic Regression Workflow: Jupyter Notebooks, Git View Code on GitHub Try the Tool You can upload a sample CSV or download one here.\n","permalink":"https://sebastiankindl.at/projects/lead-score/","summary":"A classification model designed to identify high-potential leads and maximize sales efficiency.","title":"Predictive Lead Scoring"}]